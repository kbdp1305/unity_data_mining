{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd    # digunakan untuk manipulasi dan analisis data\n",
    "import matplotlib.pyplot as plt   # digunakan untuk visualisasi data\n",
    "# import seaborn as sns   # juga digunakan untuk visualisasi data\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import(   # digunakan untuk validasi silang dan hyperparameter tuning\n",
    "    train_test_split,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    ")\n",
    "\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import pyarrow.feather as feather\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string\n",
    "import sys\n",
    "# sys.path.append('/content/indonlu')\n",
    "# from transformers import  BertConfig, BertTokenizer,BertForSequenceClassification\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Model IndoBERT\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.lang.id import Indonesian\n",
    "from spacy.lang.id.stop_words import STOP_WORDS\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "# from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "# from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader\n",
    "# from indonlu.utils.forward_fn import forward_sequence_classification\n",
    "# from indonlu.utils.metrics import document_sentiment_metrics_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kamus_alay_url='https://drive.google.com/uc?id=14M5i-Dd_oFCCJf4avgTEQeZLO5PCKCzy'\n",
    "kamus_alay_path=r'C:\\Lomba\\UNITY UNY\\Data Mining\\resources\\kamus_alay.csv'\n",
    "kamus_singkatan_url='https://drive.google.com/uc?id=1f1aOazCYJXCnhg7Brf5GelE7yE8-QflK'\n",
    "kamus_singkatan_path=r'C:\\Lomba\\UNITY UNY\\Data Mining\\resources/kamus_singkatan.csv'\n",
    "gdown.download(kamus_alay_url, kamus_alay_path, quiet=False)\n",
    "gdown.download(kamus_singkatan_url, kamus_singkatan_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kesehatan=pd.read_csv(r\"C:\\Lomba\\UNITY UNY\\Data Mining\\scraping\\dataset\\Direktorat Kesehatan, Perhubungan, dan PLN.csv\")\n",
    "df_jenderal=pd.read_csv(r\"C:\\Lomba\\UNITY UNY\\Data Mining\\scraping\\dataset\\jenderal kesehatan.csv\") \n",
    "df_ristek=pd.read_csv(r\"C:\\Lomba\\UNITY UNY\\Data Mining\\scraping\\dataset\\Kementrian Ristek.csv\")\n",
    "df_sosial=pd.read_csv(r\"C:\\Lomba\\UNITY UNY\\Data Mining\\scraping\\dataset\\sosial,polisi,kemendikbud.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df_kesehatan,df_jenderal,df_ristek,df_sosial],ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(subset='detail_keluhan', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_stopwords = [\n",
    "    \"tidak\",\n",
    "    \"bukan\",\n",
    "    \"jangan\",\n",
    "    \"tak\",\n",
    "    \"belum\",\n",
    "    \"tidak ada\",\n",
    "    \"tidak pernah\",\n",
    "    \"justru\",\n",
    "    \"malah\",\n",
    "    \"hanya\",\n",
    "    \"saja\",\n",
    "    \"sungguh\",\n",
    "    \"amat\",\n",
    "    \"terlalu\",\n",
    "    \"benar-benar\",\n",
    "    \"sangat\",\n",
    "    \"sekali\",\n",
    "    \"ternyata\",\n",
    "    \"tadinya\",\n",
    "    \"bahwa\",\n",
    "    \"apabila\",\n",
    "    \"jika\",\n",
    "    \"jikalau\",\n",
    "    \"seandainya\",\n",
    "    \"sekiranya\",\n",
    "    \"sejak\",\n",
    "    \"semenjak\",\n",
    "    \"sewaktu\",\n",
    "    \"sebelum\",\n",
    "    \"sesudah\",\n",
    "    \"selesai\",\n",
    "    \"ketika\",\n",
    "    \"kemudian\",\n",
    "    \"setelah\",\n",
    "    \"sementara\",\n",
    "    \"begitu\",\n",
    "    \"demikian\",\n",
    "    \"seperti\",\n",
    "    \"serupa\",\n",
    "    \"mirip\",\n",
    "    \"sama\",\n",
    "    \"seolah-olah\",\n",
    "    \"akan\",\n",
    "    \"mau\",\n",
    "    \"mesti\",\n",
    "    \"harus\",\n",
    "    \"perlu\",\n",
    "    \"patut\",\n",
    "    \"boleh\",\n",
    "    \"dapat\",\n",
    "    \"bisa\",\n",
    "    \"mampu\",\n",
    "    \"sebaiknya\",\n",
    "    \"seharusnya\",\n",
    "    \"sempat\",\n",
    "    \"tahu\",\n",
    "    \"tahulah\",\n",
    "    \"ketahui\",\n",
    "    \"tandanya\",\n",
    "    \"yaitu\",\n",
    "    \"adalah\",\n",
    "    \"namun\",\n",
    "    \"ga\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kamus_alay=pd.read_csv(kamus_alay_path)\n",
    "kamus_alay=kamus_alay.rename(columns = {'slang' : 'kamus_slang' , 'formal' : 'kamus_perbaikan'})\n",
    "# Rekonstruksi data sebagai 'dict'\n",
    "slang_mapping = dict(zip(kamus_alay['kamus_slang'], kamus_alay['kamus_perbaikan']))\n",
    "kamus_singkatan = pd.read_csv(kamus_singkatan_path, header=None, names=['sebelum_perbaikan', 'setelah_perbaikan'],delimiter=';')\n",
    "singkatan_mapping=dict(zip(kamus_singkatan['sebelum_perbaikan'],kamus_singkatan['setelah_perbaikan']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan kamus kata gaul Salsabila\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import  StopWordRemoverFactory\n",
    "import emoji\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopwords = stopword_factory.get_stop_words()\n",
    "# List of words with negation meaning\n",
    "data = emoji.EMOJI_DATA\n",
    "\n",
    "# Remove negation words from stopwords\n",
    "stopwords = set(stopwords).difference(excluded_stopwords)\n",
    "nlp = Indonesian()\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teks dengan emoji\n",
    "text_with_emoji = \"Saat kamu merenungkan tentang kehilangan yang pernah kamu alami, luka-luka itu terasa kembali dalam ingatan. ðŸ’”ðŸŒ¼ #RememberingLoss\"\n",
    "\n",
    "# Fungsi untuk mengganti emoji dengan representasi ASCII sesuai dengan data emoji\n",
    "def replace_emoji_with_ascii(text, emoji_data, language='id'):\n",
    "    for emoji, translations in emoji_data.items():\n",
    "        if language in translations:\n",
    "            text = text.replace(emoji, translations[language])\n",
    "#             text=text.replace(':','')\n",
    "#             text=text.replace('_',' ')\n",
    "#             text = text.replace(':', '').replace('_', ' ')\n",
    "\n",
    "\n",
    "\n",
    "    return text\n",
    "text_with_ascii = replace_emoji_with_ascii(text_with_emoji, data, language='id')\n",
    "\n",
    "text_without_extra_spaces=text_with_emoji.strip().lstrip()\n",
    "\n",
    "# Menghilangkan spasi di awal dan akhir teks\n",
    "text_without_extra_spaces = text_without_extra_spaces.strip()\n",
    "def process_tweet(tweet) :\n",
    "  tweet=tweet.lower()\n",
    "  tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "  tweet=replace_emoji_with_ascii(tweet,data)\n",
    "  tokens = tweet.split()\n",
    "  tweet_tokens = []\n",
    "  for ele in tokens:\n",
    "    ele_kamus = kamus_singkatan.get(ele, ele)\n",
    "    ele_slang = slang_mapping.get(ele_kamus, ele_kamus)\n",
    "    tweet_tokens.append(ele_slang)\n",
    "\n",
    "  tweet = ' '.join(tweet_tokens)\n",
    "  tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "  tweet = re.sub(r'#([^\\s]+)', '', tweet)\n",
    "  tweet=re.sub(r'\\d+', '', tweet)\n",
    "  tweet = tweet.strip('\\'\"')\n",
    "  tweet = tweet.lstrip('\\'\"')\n",
    "\n",
    "  tweet = \"\".join([char for char in tweet if char not in string.punctuation])\n",
    "\n",
    "  doc = nlp(tweet)\n",
    "  tokens = [token.text for token in doc]\n",
    "      # Hapus stopwords dari tokens\n",
    "  filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "  tweet = ' '.join(filtered_tokens)\n",
    "\n",
    "  tweet=stemmer.stem(tweet) \n",
    "\n",
    "\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_detail_keluhan']=df['detail_keluhan'].apply(lambda x: process_tweet(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Lomba\\UNITY UNY\\Data Mining\\scraping\\dataset\\cleaned_detail_keluhan.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['detail_keluhan'],df['instansi'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_tsv(X_train, X_test, y_train, y_test,train_name,test_name) :\n",
    "    # Training data\n",
    "    train_df = pd.DataFrame({\"detail_keluhan\": X_train, \"label\": y_train})\n",
    "    train_df.to_csv(train_name, sep=\"\\t\", index=False,header=False)\n",
    "\n",
    "    # Test data\n",
    "    test_df = pd.DataFrame({\"detail_keluhan\": X_test, \"label\": y_test})\n",
    "    test_df.to_csv(test_name,  sep='\\t',index=False,header=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name=r\"train_detail_keluhan.tsv\"\n",
    "test_name=r\"test_detail_keluhan.tsv\"\n",
    "save_as_tsv(X_train, X_test, y_train, y_test,train_name,test_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tsv(train_path,test_path) :\n",
    "    train_df=pd.read_csv(train_path,sep='\\t',header=None)      \n",
    "    test_df=pd.read_csv(test_path,sep='\\t',header=None)\n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load_tsv(train_name,test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(r'C:\\Lomba\\UNITY UNY\\Data Mining\\scraping\\dataset\\cleaned_judul_laporan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier  # Import XGBClassifier\n",
    "from lightgbm import LGBMClassifier  # Import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Create a CatBoostClassifier instance\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn import svm \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_data,seed=42,path=None) : \n",
    "    models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "#     'Decision Tree': DecisionTreeClassifier(random_state=seed),\n",
    "    'Random Forest': RandomForestClassifier(random_state=seed) ,\n",
    "    'XGBoost': XGBClassifier(random_state=seed),\n",
    "    'LightGBM': LGBMClassifier(random_state=seed),\n",
    "    'CatBoost' : CatBoostClassifier(random_seed=42,iterations=1000,verbose=0)\n",
    "}\n",
    "    train_data=train_data.dropna()  \n",
    "    X = train_data['cleaned_judul_laporan']\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_data['instansi_encoded'] = label_encoder.fit_transform(train_data['instansi'])\n",
    "    y = train_data['instansi_encoded']\n",
    "    label_mapping = label_encoder.classes_  \n",
    "    save_models= {\n",
    "        'Model': [],\n",
    "        'Label_Mapping': [],\n",
    "        'Accuracy': [],\n",
    "        'Classification_Report': []\n",
    "      \n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a TfidfVectorizer with bigrams\n",
    "    tfidf_vectorizer = TfidfVectorizer( max_features=5000)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    # Transform your test data using the same vectorizer and make predictions\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    # Train a sentiment analysis model (Logistic Regression in this example)\n",
    "    for i,j in models.items() : \n",
    "        model = j\n",
    "        model.fit(X_train_tfidf.toarray(), y_train)\n",
    "        y_pred = model.predict(X_test_tfidf.toarray())\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        # classification_rep = classification_report(y_test, y_pred)\n",
    "        # print(\"Classification Report:\\n\", classification_rep)\n",
    "        y_test_mapped = [label_mapping[label] for label in y_test]\n",
    "        y_pred_mapped = [label_mapping[label] for label in y_pred]\n",
    "        \n",
    "        # Generate classification report with reversed label names\n",
    "        classification_rep = classification_report(y_test_mapped, y_pred_mapped)\n",
    "        print(\"Classification Report:\\n\", classification_rep)\n",
    "        model_name=i\n",
    "        save_models['Model'].append(model)\n",
    "        save_models['Label_Mapping'].append(label_mapping)\n",
    "        save_models['Accuracy'].append(accuracy)\n",
    "        save_models['Classification_Report'].append(classification_rep) \n",
    "        model_path = path + str(model_name) + '.pkl'\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        pickle.dump(label_mapping, open(path + f'{i}_label_mapping.pkl', 'wb'))\n",
    "        pickle.dump(classification_rep, open(path +f'{i}_classification_rep.pkl', 'wb'))\n",
    "        pickle.dump(accuracy, open(path + f'{i}_accuracy.pkl', 'wb'))\n",
    "    frame=pd.DataFrame(save_models)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:/Lomba/UNITY UNY/Data Mining/results/machine learning/\"\n",
    "frame=training(train_df,path=path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Lomba\\UNITY UNY\\Data Mining\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data, self.labels = self.load_data(file_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        data = []\n",
    "        labels = []\n",
    "        label_mapping = {\"Kementerian Sosial\": 1, \"Divisi Pemasaran dan Pelayanan Pelanggan\": 2, \"Kementerian Pendidikan, Kebudayaan, Riset, dan Teknologi\": 3,'Kepolisian Negara Republik Indonesia':4,'Direktorat Jenderal Perhubungan Darat':5, 'Kementerian Komunikasi dan Informatika' : 6, 'Direktorat Jenderal Tenaga Kesehatan' :7}  # Define your label mapping here\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "\n",
    "                if len(parts) == 2:\n",
    "                    text, label = parts\n",
    "                    data.append(text)\n",
    "                    labels.append(label_mapping[label])\n",
    "        return data, labels\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your datasets\n",
    "train_dataset_path = 'train_detail_keluhan.tsv'\n",
    "valid_dataset_path = 'test_detail_keluhan.tsv'\n",
    "\n",
    "def load_tsv(batch_size=4) : \n",
    "# Instantiate the custom datasets\n",
    "    train_dataset = CustomDataset(train_dataset_path)\n",
    "    valid_dataset = CustomDataset(valid_dataset_path)\n",
    "\n",
    "    # Define the data loaders\n",
    "   \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "    return train_loader,valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training(model,tokenizer,train_loader,valid_loader,epoch) :\n",
    "    model2=model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model2.to(device)\n",
    "    optimizer = Adam(model2.parameters(), lr=3e-6)  # Define the optimizer\n",
    "   \n",
    "\n",
    "    num_epochs = epoch# Adjust the number of epochs as needed\n",
    "    history2 = defaultdict(list)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        model2.train()\n",
    "\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "\n",
    "        for batch in train_loader_tqdm:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            loss = loss_fn(logits,labels)\n",
    "\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        train_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        train_f1_score = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        train_error = 1 - train_accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {epoch_loss / len(train_loader)}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Training Accuracy: {train_accuracy}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Training F1 Score: {train_f1_score}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Training Error: {train_error}\")\n",
    "\n",
    "        if valid_loader:\n",
    "            model2.eval()\n",
    "            validation_loss = 0.0\n",
    "            all_predictions = []\n",
    "            all_labels = []\n",
    "\n",
    "            valid_loader_tqdm = tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader_tqdm:\n",
    "                    input_ids = batch[\"input_ids\"].to(device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                    outputs = model2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    logits = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "                    loss = loss_fn(logits,labels)\n",
    "                    validation_loss += loss.item()\n",
    "\n",
    "                    predictions = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_predictions.extend(predictions)\n",
    "                    all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            valid_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "            valid_f1_score = f1_score(all_labels, all_predictions, average='weighted')\n",
    "            valid_error = 1 - valid_accuracy\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {validation_loss / len(valid_loader)}\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Validation Accuracy: {valid_accuracy}\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Validation F1 Score: {valid_f1_score}\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Validation Error: {valid_error}\")\n",
    "\n",
    "        # Store metrics in history2\n",
    "        history2['train_loss'].append(epoch_loss / len(train_loader))\n",
    "        history2['train_accuracy'].append(train_accuracy)\n",
    "        history2['train_f1_score'].append(train_f1_score)\n",
    "        history2['train_error'].append(train_error)\n",
    "        history2['valid_loss'].append(validation_loss / len(valid_loader))\n",
    "        history2['valid_accuracy'].append(valid_accuracy)\n",
    "        history2['valid_f1_score'].append(valid_f1_score)\n",
    "        history2['valid_error'].append(valid_error)\n",
    "    return model2,history2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDOBERTWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\")\n",
    "model2 = AutoModel.from_pretrained(\"indolem/indobertweet-base-uncased\")\n",
    "train_loader,valid_loader=load_tsv()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,history=training(model2,tokenizer2,train_loader,valid_loader,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = r\"C:/Lomba/UNITY UNY/Data Mining/results/\"\n",
    "historyname = \"history_detail_laporan_baseline.pkl\"\n",
    "model_name = \"baseline_detail_laporan.pth\"\n",
    "\n",
    "def save_state(model,history,path,model_name,history_name) :\n",
    "    torch.save(model.state_dict(), path+model_name)\n",
    "    with open(path+historyname, 'wb') as f:\n",
    "        pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state(model,history,paths,model_name,historyname) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDOBERT BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "model2 = AutoModel.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "train_dataset_path = 'train_detail_keluhan.tsv'\n",
    "valid_dataset_path = 'test_detail_keluhan.tsv'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = 'train_detail_keluhan.tsv'\n",
    "valid_dataset_path = 'test_detail_keluhan.tsv'\n",
    "train_loader,valid_loader=load_tsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [04:02<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Training Loss: 2.9552060254425703\n",
      "Epoch 1/15 - Training Accuracy: 0.24480054551653596\n",
      "Epoch 1/15 - Training F1 Score: 0.2407816404082465\n",
      "Epoch 1/15 - Training Error: 0.755199454483464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:24<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Validation Loss: 1.679568991712902\n",
      "Epoch 1/15 - Validation Accuracy: 0.4659400544959128\n",
      "Epoch 1/15 - Validation F1 Score: 0.44757722846912273\n",
      "Epoch 1/15 - Validation Error: 0.5340599455040872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [05:39<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Training Loss: 1.7429232801988275\n",
      "Epoch 2/15 - Training Accuracy: 0.48312308216842825\n",
      "Epoch 2/15 - Training F1 Score: 0.4732214997117656\n",
      "Epoch 2/15 - Training Error: 0.5168769178315717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:35<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Validation Loss: 1.1435928547997838\n",
      "Epoch 2/15 - Validation Accuracy: 0.6362397820163488\n",
      "Epoch 2/15 - Validation F1 Score: 0.635508366704496\n",
      "Epoch 2/15 - Validation Error: 0.3637602179836512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:13<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Training Loss: 1.325083259713255\n",
      "Epoch 3/15 - Training Accuracy: 0.6137061029662462\n",
      "Epoch 3/15 - Training F1 Score: 0.610583693504275\n",
      "Epoch 3/15 - Training Error: 0.38629389703375383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:37<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Validation Loss: 1.0251701405352873\n",
      "Epoch 3/15 - Validation Accuracy: 0.6798365122615804\n",
      "Epoch 3/15 - Validation F1 Score: 0.6792790844590907\n",
      "Epoch 3/15 - Validation Error: 0.3201634877384196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:29<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Training Loss: 1.0856336096202706\n",
      "Epoch 4/15 - Training Accuracy: 0.6651892260484146\n",
      "Epoch 4/15 - Training F1 Score: 0.6628423409138391\n",
      "Epoch 4/15 - Training Error: 0.3348107739515854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:36<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Validation Loss: 0.928121423288046\n",
      "Epoch 4/15 - Validation Accuracy: 0.6989100817438693\n",
      "Epoch 4/15 - Validation F1 Score: 0.7004170892633457\n",
      "Epoch 4/15 - Validation Error: 0.30108991825613074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:16<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Training Loss: 0.9496897472156217\n",
      "Epoch 5/15 - Training Accuracy: 0.7190589839754518\n",
      "Epoch 5/15 - Training F1 Score: 0.7175860557399671\n",
      "Epoch 5/15 - Training Error: 0.28094101602454824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:35<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Validation Loss: 0.9195316712493482\n",
      "Epoch 5/15 - Validation Accuracy: 0.7166212534059946\n",
      "Epoch 5/15 - Validation F1 Score: 0.7158206469822188\n",
      "Epoch 5/15 - Validation Error: 0.2833787465940054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:23<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Training Loss: 0.8633671303997222\n",
      "Epoch 6/15 - Training Accuracy: 0.7439481759290828\n",
      "Epoch 6/15 - Training F1 Score: 0.7433406130619197\n",
      "Epoch 6/15 - Training Error: 0.2560518240709172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:37<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Validation Loss: 0.9199577676835875\n",
      "Epoch 6/15 - Validation Accuracy: 0.7193460490463215\n",
      "Epoch 6/15 - Validation F1 Score: 0.7214502167501388\n",
      "Epoch 6/15 - Validation Error: 0.2806539509536785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:22<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Training Loss: 0.7584422358406241\n",
      "Epoch 7/15 - Training Accuracy: 0.7783839072621889\n",
      "Epoch 7/15 - Training F1 Score: 0.7780105539641884\n",
      "Epoch 7/15 - Training Error: 0.22161609273781113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:37<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Validation Loss: 0.9451922381830746\n",
      "Epoch 7/15 - Validation Accuracy: 0.717983651226158\n",
      "Epoch 7/15 - Validation F1 Score: 0.7212704140953887\n",
      "Epoch 7/15 - Validation Error: 0.28201634877384196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:15<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Training Loss: 0.660935744809587\n",
      "Epoch 8/15 - Training Accuracy: 0.7984998295260826\n",
      "Epoch 8/15 - Training F1 Score: 0.7979628819626509\n",
      "Epoch 8/15 - Training Error: 0.20150017047391744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:36<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Validation Loss: 0.9483409434754361\n",
      "Epoch 8/15 - Validation Accuracy: 0.726158038147139\n",
      "Epoch 8/15 - Validation F1 Score: 0.7281676028940838\n",
      "Epoch 8/15 - Validation Error: 0.273841961852861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:19<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Training Loss: 0.615162962113922\n",
      "Epoch 9/15 - Training Accuracy: 0.8189566996249574\n",
      "Epoch 9/15 - Training F1 Score: 0.818652766463182\n",
      "Epoch 9/15 - Training Error: 0.18104330037504257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:35<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Validation Loss: 0.9042004207155475\n",
      "Epoch 9/15 - Validation Accuracy: 0.7316076294277929\n",
      "Epoch 9/15 - Validation F1 Score: 0.7315693220384114\n",
      "Epoch 9/15 - Validation Error: 0.2683923705722071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:14<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Training Loss: 0.5661129708608758\n",
      "Epoch 10/15 - Training Accuracy: 0.8233890214797136\n",
      "Epoch 10/15 - Training F1 Score: 0.8230970535389617\n",
      "Epoch 10/15 - Training Error: 0.1766109785202864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:36<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Validation Loss: 0.9399089456580797\n",
      "Epoch 10/15 - Validation Accuracy: 0.7465940054495913\n",
      "Epoch 10/15 - Validation F1 Score: 0.7476884204788725\n",
      "Epoch 10/15 - Validation Error: 0.2534059945504087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:14<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Training Loss: 0.5057213867200916\n",
      "Epoch 11/15 - Training Accuracy: 0.8441868394135698\n",
      "Epoch 11/15 - Training F1 Score: 0.8439883141043558\n",
      "Epoch 11/15 - Training Error: 0.15581316058643024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:36<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Validation Loss: 0.9899955018280826\n",
      "Epoch 11/15 - Validation Accuracy: 0.7425068119891008\n",
      "Epoch 11/15 - Validation F1 Score: 0.7468490834894839\n",
      "Epoch 11/15 - Validation Error: 0.25749318801089915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:23<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Training Loss: 0.48849736383936254\n",
      "Epoch 12/15 - Training Accuracy: 0.8537333787930447\n",
      "Epoch 12/15 - Training F1 Score: 0.8536813610089855\n",
      "Epoch 12/15 - Training Error: 0.14626662120695533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:36<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Validation Loss: 0.9870298553998272\n",
      "Epoch 12/15 - Validation Accuracy: 0.7343324250681199\n",
      "Epoch 12/15 - Validation F1 Score: 0.734747308404674\n",
      "Epoch 12/15 - Validation Error: 0.2656675749318801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Training Loss: 0.41350964792168726\n",
      "Epoch 13/15 - Training Accuracy: 0.8728264575519945\n",
      "Epoch 13/15 - Training F1 Score: 0.8730060953344403\n",
      "Epoch 13/15 - Training Error: 0.1271735424480055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:37<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Validation Loss: 1.05404192928575\n",
      "Epoch 13/15 - Validation Accuracy: 0.7356948228882834\n",
      "Epoch 13/15 - Validation F1 Score: 0.7364833632857669\n",
      "Epoch 13/15 - Validation Error: 0.26430517711171664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:21<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Training Loss: 0.4052598372491791\n",
      "Epoch 14/15 - Training Accuracy: 0.8772587794067508\n",
      "Epoch 14/15 - Training F1 Score: 0.8774556134586762\n",
      "Epoch 14/15 - Training Error: 0.12274122059324921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:37<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Validation Loss: 1.0275492244697448\n",
      "Epoch 14/15 - Validation Accuracy: 0.7520435967302452\n",
      "Epoch 14/15 - Validation F1 Score: 0.7548138844540772\n",
      "Epoch 14/15 - Validation Error: 0.24795640326975477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 734/734 [06:21<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Training Loss: 0.37115685087664746\n",
      "Epoch 15/15 - Training Accuracy: 0.8912376406409819\n",
      "Epoch 15/15 - Training F1 Score: 0.8913601827927589\n",
      "Epoch 15/15 - Training Error: 0.10876235935901812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:36<00:00,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Validation Loss: 1.0296774047825712\n",
      "Epoch 15/15 - Validation Accuracy: 0.7520435967302452\n",
      "Epoch 15/15 - Validation F1 Score: 0.7546788394534998\n",
      "Epoch 15/15 - Validation Error: 0.24795640326975477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_base,history_base=training(model2,tokenizer2,train_loader,valid_loader,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = r\"C:/Lomba/UNITY UNY/Data Mining/results/\"\n",
    "historyname = \"Indobert_baseline_history_detail_laporan_baseline.pkl\"\n",
    "model_name = \"Indobert_baseline_detail_laporan.pth\"\n",
    "\n",
    "def save_state(model,history,path,model_name,history_name) :\n",
    "    torch.save(model.state_dict(), path+model_name)\n",
    "    with open(path+historyname, 'wb') as f:\n",
    "        pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state(model_base,history_base,paths,model_name,historyname)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
